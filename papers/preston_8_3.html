
<!-- saved from url=(0105)https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./preston_8_3_files/analytics.js" type="text/javascript"></script>
<script type="text/javascript">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app209.us.archive.org';v.server_ms=139;archive_analytics.send_pageview({});});</script>
<script type="text/javascript" src="./preston_8_3_files/bundle-playback.js" charset="utf-8"></script>
<script type="text/javascript" src="./preston_8_3_files/wombat.js" charset="utf-8"></script>
<script type="text/javascript">
  __wm.init("https://web.archive.org/web");
  __wm.wombat("http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm","20130307063758","https://web.archive.org/","web","/_static/",
	      "1362638278");
</script>
<link rel="stylesheet" type="text/css" href="./preston_8_3_files/banner-styles.css">
<link rel="stylesheet" type="text/css" href="./preston_8_3_files/iconochive.css">
<!-- End Wayback Rewrite JS Include -->

<title>Journal of Statistics Education, V8N2:Preston</title>
</head>
<body bgcolor="#FFFFFF" data-new-gr-c-s-check-loaded="14.1036.0" data-gr-ext-installed=""><!-- BEGIN WAYBACK TOOLBAR INSERT -->
<style type="text/css">
body {
  margin-top:0 !important;
  padding-top:0 !important;
  /*min-width:800px !important;*/
}
</style>
<script>__wm.rw(0);</script>
<div id="wm-ipp-base" lang="en" style="display: block; direction: ltr;">
</div><div id="wm-ipp-print">The Wayback Machine - https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm</div>
<div id="donato" style="position:relative;width:100%;">
  <div id="donato-base">
    <iframe id="donato-if" src="./preston_8_3_files/donate.html" scrolling="no" frameborder="0" style="width:100%; height:100%">
    </iframe>
  </div>
</div><script type="text/javascript">
__wm.bt(650,27,25,2,"web","http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm","20130307063758",1996,"/_static/",["/_static/css/banner-styles.css?v=omkqRugM","/_static/css/iconochive.css?v=qtvMKcIJ"], "False");
  __wm.rw(1);
</script>
<!-- END WAYBACK TOOLBAR INSERT -->

<h1>Teaching Prediction Intervals</h1>

<p>Scott Preston<br>
State University of New York, College at Oswego</p>

<p>
<cite>Journal of Statistics Education</cite> v.8, n.3 (2000)

</p><p>Copyright (c) 2000 by Scott Preston, all rights
reserved. This text may be freely shared among individuals,
but it may not be republished in any medium without express
written consent from the author and advance notification of
the editor.</p> 

<p></p><hr><p></p>

<p><strong>Key Words:</strong>   Confidence interval;
Descriptive statistic; Error reduction; Normal
distribution; Percentile.</p> 

<h2>Abstract</h2>


<p>Teaching prediction intervals to introductory audiences
presents unique opportunities.  In this article I present a
strategy for involving students in the development of a
nonparametric prediction interval.  Properties of the
resulting procedure, as well as related concepts and
similar procedures that appear throughout statistics, may
be illustrated and investigated within the concrete context
of the data. I suggest a generalization of the usual normal
theory prediction interval.  This generalization, in tandem
with the nonparametric method, results in an approach to
prediction that may be systematically deployed throughout a
course in introductory statistics.</p>


<h1>1. Introduction</h1>


<p>1 The prediction interval generally occupies a rather
small niche in introductory statistics courses. This is
unfortunate because there are meaningful applications for
prediction intervals, as well as sound pedagogical reasons
for covering the topic thoroughly. Recent articles by
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Whitmore">Whitmore (1986)</a>, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Scheuer">Scheuer (1990)</a>, and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Vardeman92">Vardeman (1992)</a> argue
in favor of placing more emphasis on the prediction
interval.  In this article I present strategies for
teaching the prediction interval effectively, efficiently,
and comprehensively.  My exposition largely targets the
non-calculus-based introductory applied statistics course;
the material is easily adapted for other audiences.</p>

<p>2 One especially attractive opportunity afforded by
teaching prediction intervals is that with minimal
guidance—and no formal background in
probability—<i>students</i> can <i>develop</i> the
nonparametric prediction interval (treated in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section3">Section 3</a>). 
The development is visual in nature; the method is applied
without resorting to a formula.</p>

<p>3 Prediction intervals are generally treated as
inferential methods, but there are good reasons for
integrating the nonparametric prediction interval into
coverage of descriptive statistics. Students readily agree
that a procedure that predicts future observations with a
given level of confidence ought to predict the data on hand
at approximately that level (the slight difference is
clarified by the visual device used to develop the
nonparametric prediction interval). Consequently, a
prediction interval, when plotted to accompany graphically
displayed data, is validated by the data. Further,
percentiles, an important descriptive measure, may be
associated with prediction intervals. Presenting the two together yields a consistent approach to forming statistical
intervals.</p>

<p>4 The nonparametric prediction interval also offers
advantages when discussing inferential methods. This
interval introduces the idea of inference based on order
without requiring combinatorics, nor a parameter to be
estimated. It shares properties with other statistical
intervals, and some generalizations about statistical
intervals are illustrated quickly, and concretely, via
appeal to the nonparametric prediction interval. On the
other hand, obtaining a prediction interval in conjunction
with a confidence interval (CI) disavows a number of
students of a misconception about CIs, namely that
"<i>C%</i>   of the data lies between <i>L</i> and
<i>U</i>."</p>

<p>5 Applications for prediction intervals are found in a
number of settings, including business and quality control;
see, for instance, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Patel">Patel (1989, p. 2396)</a>, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Scheaffer">Scheaffer and McClave (1990, p. 292)</a>, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Hahn91">Hahn and Meeker (1991, p. 31)</a>,
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Vardeman94">Vardeman (1994, p. 347)</a>, and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Neter">Neter, Kutner, Nachtsheim, and
Wasserman (1996, pp. 65-66)</a>. In contrast to textbook
coverage of prediction intervals, which is usually
restricted to regression situations (in the limited sense
of regression as curve-fitting), these applications exist
for the variety of standard models covered in introductory
courses. Prediction intervals are obtained in identical
fashion for each of these models, reinforcing the
perspective of a symmetric approach to a variety of
situations.</p>

<p>6 This article focuses on strategies for teaching
prediction intervals, and on ways to use prediction
intervals to introduce and illustrate other important
statistical tools and concepts in a fairly benign setting.
The material in this article was developed to serve an
audience that is not mathematically sophisticated, is prone
to choose blind computation over reflection and
interpretation, and is quantitatively timid (and
consequently often intimidated). My students have average
algebra skills; a sizable minority have taken a single
semester of calculus—generally business calculus. They
need considerable guidance developing an appreciation for
the systematic fashion in which statistical methods are
deployed.  They need simple, concrete examples that
demonstrate the nature of inferential statements.  My
premise in designing these materials was to illustrate
statistical concepts, and relationships among them, in
direct reference to observed data, and to develop
techniques that are used the same way in a variety of
models.</p>

<p>7 In what follows I discuss methods that succeed in my
classroom.  They succeed in part by simply allowing
students to take some small amount of ownership in the
theory; in part by taking advantage of prediction's direct
association with the data, as a consequence making concrete
the illustration of more sophisticated concepts; and in
part by developing a systematic framework for constructing
statistical intervals—one that may be applied in a
variety of situations.  My aim is to convince you that a
comprehensive treatment of prediction intervals addresses
fundamental statistical issues—issues that may be 
introduced early in an introductory statistics course, then
revisited, extended, elaborated upon, and contrasted with
other statistical tools.</p>




<h2>1.1 Organization </h2>

<p>8 Formal definitions are established in  <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section2">Section 2</a>.  In <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section3">Section 3</a>, I introduce the one-sample
nonparametric prediction interval.  The relationship
between this prediction interval and percentiles is
pursued.  An application for this prediction interval in a
descriptive treatment of simple linear regression is also
presented.  In <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section4">Section 4</a>, I discuss
a generalization of the normal theory prediction interval
covered in many textbooks.  In <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section5">Section
5</a>, the nonparametric prediction interval and the normal
theory prediction interval are compared.  Illustrations
throughout are in terms of two-sided intervals; treatment
of one-sided intervals is analogous (students will likely
suggest one-sided procedures when given the chance to work
with the nonparametric prediction interval). Details on the
data-generating activity mentioned in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section31">Section 3.1</a>, Minitab macros for
obtaining prediction intervals and percentiles, and some
remarks on textbook treatments of prediction, are included
in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#AppendixA">Appendices A</a>, <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#AppendixB">B</a>, and <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#AppendixC">C</a>,
respectively.</p>


<p>9 Different courses and different instructors address
topics in any number of sequences.  In my courses I
typically cover the material in this article as
follows.</p>

<ul>

<p></p><li> The nonparametric prediction interval of 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section3">Section 3</a>
very near the onset of the course, as part of a unit on
descriptive and graphical methods.  Prediction intervals
make a first appearance just prior to percentiles.<p></p>

<p></p></li><li>
The probability prediction interval of <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section21">Section 2.1</a> during a
subsequent (and short) unit on probability.<p></p>

<p></p></li><li> A substantial portion of the remaining course time
is allocated to coverage of both descriptive and
inferential tools for a variety of models involving a
quantitative response (one-sample, multi-sample, and
regression settings). Because properties and interpretation
of prediction intervals are most easily demonstrated with
the one-sample nonparametric prediction interval of 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section31">Section 3.1</a>, I review the method before moving on to the normal
theory prediction interval. At this point I explicitly
present the formal definition of a prediction interval
stated by <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation2">(2)</a> of 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section22">Section 2.2</a>.  I cover the normal theory
prediction interval of <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section4">Section 4</a> in each of the standard
models, positioning the nonparametric prediction interval
as an alternative for situations in which the normal theory
prediction interval is inappropriate.<p></p>

</li></ul> I do not explicitly treat the finer points on
robustness issues stated in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section34">Sections 3.4</a> and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section42">4.2</a>.  However,
proper use and interpretation of the prediction interval is
a semester-long goal. My coverage of prediction takes very
little time out of an already busy semester, while allowing
for efficiencies along the way.<p></p>

<h2>1.2 Notation </h2>

<p>10 Throughout this article, I've attempted to distinguish
formal notation as it is used to express generalized
results.  For the most part this is not the notation I use
with students.  They are expected instead to use the ideas
presented in class, and to visualize situations to motivate
solutions and interpretations, as in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure1">Figures 1</a>, <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure2">2</a>, and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure3">3</a>. 
I do make use of notation utilized in an accompanying
textbook (for, say, percentiles), with but one exception:
for normal data my students learn 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> in conjunction with
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation7">(7)</a> in place of expressions such as 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation5">(5)</a>.</p>


<h1><a name="Section2">2. Definitions</a></h1>

<p>11 Assume that a continuous response variable <i>Y</i>,
perhaps a function of explanatory variables, is randomly
sampled from a population.</p>


<h2><a name="Section21">2.1 Probability Prediction
Intervals</a></h2>

<p>12 For <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">

between 0 and 1 (an error rate), let
<i>y</i><sub><i>L</i></sub> and <i>y</i><sub><i>U</i></sub>
denote the 

(<img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">/2) × 100<sup>th</sup> and (1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">/2) × 100<sup>th</sup>

percentiles, respectively, of the population distribution
for <i>Y</i>. Then a (1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">) × 100%

<i>probability prediction interval</i> (PPI) for the
outcome <i>Y</i> is given by  (<i>y</i><sub><i>L</i></sub>,
<i>y</i><sub><i>U</i></sub>). A population distribution is
often conveniently expressed in mathematical terms by a
density function. The most commonly treated of these
mathematical idealizations is the normal distribution.  For
univariate normal data, mean <img width="16" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img8.gif" alt="$\mu$">
and standard deviation <img width="16" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img9.gif" alt="$\sigma$">,
the PPI for a randomly selected observation is</p>

<a name="Equation1"></a><div align="CENTER"><a name="Equation1"></a>

<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="186" height="38" src="./preston_8_3_files/preston.img10.gif" alt="\begin{displaymath}
\left( \mu - z_{\alpha/2} \sigma ,
\mu + z_{\alpha/2} \sigma \right).
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(1)</td></tr>
</tbody></table>

</div>

<p>Analogous statements exist for distributions other than
the normal. The probability is (1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">)

that a randomly selected observation drawn from the
specified population will fall within the bounds of such an
interval; equivalently, (1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">) × 100%

of the population falls within these bounds. Treating
prediction in this context provides practice at obtaining
appropriate percentiles—a task that is often required
when constructing confidence intervals and computing
observed significance levels.</p>


<h2><a name="Section22">2.2 Statistical Prediction
Intervals</a></h2>

<p>13 In applications, a complete specification for the
population distribution is often unknown.  Assume a random
sample,  <i>Y<sub>i</sub></i>&nbsp;, <i>i</i> = 1,…,
<i>n</i>, is to be drawn;  from this sample values <img width="26" height="44" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img13.gif" alt="$\hat{Y}_L$">
and <img width="27" height="44" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img14.gif" alt="$\hat{Y}_U$">

are to be obtained.  If, for a subsequent randomly sampled
observation <i>Y</i>,</p>

<a name="Equation2"></a><div align="CENTER"><a name="Equation2"></a>

<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="213" height="38" src="./preston_8_3_files/preston.img15.gif" alt="\begin{displaymath}
{\mbox{P}} \left[ \hat{Y}_L &lt; Y &lt; \hat{Y}_U \right] = 1 - \alpha,
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(2)</td></tr>
</tbody></table>

</div>

<p>then the observed interval 

<img width="67" height="37" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img16.gif" alt="$\left( \hat{y}_L
, \hat{y}_U \right)$">
is a (1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">) × 100%
 


<i>statistical prediction interval</i> (hereafter, simply
<i>prediction interval</i>, or PI) for <i>Y</i> (<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Patel">Patel 1989</a>).  For standard linear models where residuals are
normally distributed with unknown common variance, an
appropriate PI is formed by 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation7">(7)</a> of 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section4">Section 4</a>.  A
nonparametric alternative for the one-sample model is given by 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation3">(3)</a> of 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section3">Section 3</a>. Methods for other parameterizations
are catalogued in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Patel">Patel (1989)</a> and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Hahn91">Hahn and Meeker (1991)</a>.</p>

<p>14 Note that the specified probability of (1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">)

is a priori of observing the sample.  The (conditional)
probability that <i>Y</i> falls within the observed bounds
of the interval, 

<img width="136" height="37" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img17.gif" alt="${\mbox{P}}
\left[ \hat{y}_L &lt; Y &lt; \hat{y}_U \right]$">,

is generally unknown. In <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section3">Section 3</a>, I describe a short,
simple demonstration that exposes this distinction, the
nature of which is also at the heart of a conceptual
understanding of confidence intervals.</p>



<h1><a name="Section3">3. The Nonparametric Prediction Interval</a></h1>

<p>15 I teach the prediction interval quite early in my
courses, weaving it into a discussion of descriptive and
graphical methods as early as the first week of classes. 
By asking the right questions, I provide a touch of
direction—my students then rather quickly produce the
nonparametric PI.  Development of this PI leads naturally
to a definition of percentiles.  I also apply this PI in an
activity that demonstrates the utility of residual analysis
when quantifying the effects of adjusting for a predictor
variable in simple linear regression.</p>


<h2><a name="Section31">3.1 One-Sample Prediction
Intervals</a></h2>

<p>16 Developing a prediction procedure that satisfies 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation2">(2)</a>
is straightforward and requires of students only minimal
intuition about probability.  A simple example that I use
with classes suffices as illustration.</p>

<p>17 Each student is assigned to monitor one of a
population of manufactured components in stock. (I use
matches as surrogates for these components; each student
has measured the amount of time it takes a match to burn
out. See <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#AppendixA">Appendix A</a> for details.)  A random allocation has
put 19 of these components to use. We desire to warrant the
next (20<sup>th</sup>) component on the basis of the
failure times of the first 19 components. (Another
application uses the first 19 observations to form control
limits.) Our job is to predict, on the basis of 19 randomly
sampled observations, a subsequent  20<sup>th</sup>
observation.</p>

<p>18 Without explicitly stating what I mean by it, the
class is made aware that a "confidence figure" will
accompany the prediction.  I prompt: What is the
probability the  20<sup>th</sup> value is the smallest?
second smallest? and so on. Students readily respond that
the  20<sup>th</sup> observation is equally likely
(probability 1/20) to occupy any of positions 1 through 20
in the ordered list of all 20 observations.  I then show
them the diagram displayed in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure1">Figure 1</a>—directing their
attention to the 20 gaps formed by the first 19
observations. This suggests that 1/20 is the probability
that the 20<sup>th</sup> observation falls in any
particular gap formed by the ordered sample of 19
observations. Students quickly point out that the minimum
and maximum form 90% prediction bounds, as the subsequent
observation has 1/20 probability of falling outside either
bound.  In a similar fashion, the second smallest and
second largest values bound an 80% PI, and so forth.  
Students are quick to suggest alternate solutions, some of
which lead naturally to the development of one-sided
intervals.</p>


<p><a name="Figure1"></a></p><hr><p></p>

<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston01.gif">
<img src="./preston_8_3_files/preston01.icon.jpg">Figure 1 (10.7K gif)</a>

<p> Figure 1. Gaps Between Consecutive Ordered
Observations. This diagram draws attention to the gaps
between consecutive ordered observations in a sample of
size 19.  The next observation is equally likely to occupy
each of these 20 gaps.</p>

<p></p><hr><p></p>



<p>19 The demonstration proceeds: 19 students, from a class
of 75, are randomly sampled; the failure times of 19
components are reported in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Table1">Table 1</a>. The 90% PI, bounded by
the extreme observations, is (32.56, 87.87), the 80% PI
is (42.02, 80.37), and so on.  The data and selected PIs
are graphically depicted in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure2">Figure 2</a>. Already, without
resorting to any notation nor a formula, the relationship
(common to all statistical intervals) between interval
width and confidence level is established.</p>




<a name="Table1"><p></p><hr><p></p></a>

<p><b>Table 1. </b>Failure Times of 19 Randomly Sampled
Components</p>


<table cellspacing="1" cellpadding="6" border="4">
<tbody><tr><td align="CENTER" colspan="10">Failure time (sorted)</td>
</tr>
<tr><td align="CENTER">32.56</td>
<td align="CENTER">42.02</td>
<td align="CENTER">47.26</td>
<td align="CENTER">50.25</td>
<td align="CENTER">59.03</td>
<td align="CENTER">60.17</td>
<td align="CENTER">61.56</td>
<td align="CENTER">62.16</td>
<td align="CENTER">62.84</td>
<td align="CENTER">63.29</td>
</tr>
<tr><td align="CENTER">63.52</td>
<td align="CENTER">65.52</td>
<td align="CENTER">66.54</td>
<td align="CENTER">68.71</td>
<td align="CENTER">70.60</td>
<td align="CENTER">71.27</td>
<td align="CENTER">76.33</td>
<td align="CENTER">80.37</td>
<td align="CENTER">87.87</td>
<td align="CENTER">&nbsp;</td>
</tr>
</tbody></table>


<p><a name="Figure2"></a></p><hr><p></p>
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston02.gif">
<img src="./preston_8_3_files/preston02.icon.jpg">Figure 2 (5.4K gif)</a>

<p> Figure 2. Dotplot of Failure Times. A dotplot of the
failure time data of <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Table1">Table 1</a> is
displayed. One-sample nonparametric prediction intervals
are shown for a variety of levels.</p>

<p></p><hr><p></p>




<p>20 Formally, given a random sample of size <i>n</i> and
subsequent observation <i>Y</i>, with
<i>Y</i><sub>(<i>i</i>)</sub> denoting the
<i>i</i><sup>th</sup> order statistic among the initial
<i>n</i> observations, and <i>j</i> some index between 1
and  [<i>n</i>/2], </p>

<center>
<img width="300" height="47" src="./preston_8_3_files/preston.img27.gif" alt="\begin{displaymath}P \left[ Y_{\left( j \right)} &lt; Y &lt; Y_{\left( n + 1 - j
\right)} \right] = 1 - \frac{2j}{n+1}.
\end{displaymath}">
</center>

<p>Consequently, </p>

<!-- MATH: \begin{equation}
\left( y_{\left( j \right)}, y_{\left( n + 1 - j \right)} \right)
{\mbox{ is a }} \left( 1 - \alpha \right) \times 100 \%
{\mbox{ PI, where }} \alpha = \frac{2j}{n+1}.
\end{equation} -->

<a name="Equation3"></a><div align="CENTER"><a name="Equation3"></a>

<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="468" height="47" src="./preston_8_3_files/preston.img28.gif" alt="\begin{displaymath}
\left( y_{\left( j \right)}, y_{\left( n + 1 - j \right)} \r...
...t) \times 100 \%
{\mbox{ PI, where }} \alpha = \frac{2j}{n+1}.
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(3)</td></tr>
</tbody></table>

</div>

<p> <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Wilks">(Wilks 1941)</a>. (My students are required to visualize
and think through the argument rather than apply this sort
of notation.)</p>

<p>21 We continue with the demonstration, using the 90%
interval.  A randomly selected 20<sup>th</sup> component is
put to work and the failure time observed: It either
<i>does</i> or <i>does not</i> fall within the observed
bounds of the interval.  Retrenching, I survey the failure
times of each of the remaining 56 components (originally
unsampled): 52, or 92.86%, have failure time between the
<i>observed</i> bounds of the 90% PI.  The confidence level
of 90% is <i>not</i> the probability a subsequent value
falls between the observed extremes—90% is instead a
<i>property of the procedure</i>. (The point is made with
but a single unsampled unit.)  This is also evident from
the relative sizes of the observed gaps.  After plotting
the 19 sampled values, students agree that the gap of 2.99
between the third and fourth smallest values is less likely
to include a subsequent value than is the gap of 8.78
between the fourth and fifth smallest values. The argument
they've bought into (20 gaps of 1/20 probability each)
works only on average, not in particular cases.</p>

<p>22 An interpretation is reinforced by repeating the
demonstration.</p>

<ul>

<p></p><li> <i>On average</i> the observed bounds have .90
probability of including the next observation.<p></p>

<p></p></li><li> 90% of the time the <i>entire procedure</i> is
implemented, the subsequent  20<sup>th</sup> observation
falls within the bounds obtained from the first 19
observations.<p></p>
</li></ul>

<p>If nothing else, students are now convinced that results
of even the simplest statistical procedures must be
interpreted carefully. (As numerous opportunities arise
later, I prefer not to dwell too long on such conceptual
issues while still covering descriptive methods.)</p>

<p>23 It is worthwhile to demonstrate how matters progress
differently when the entire population is specified in
advance. (Because students obtain data prior to class, and
in my presence, I have the luxury of precomputing PPIs.) 
Here a PPI is obtained—the bounds are not subject to
sampling variability. However, knowledge of PPI bounds
implies a complete specification for the population
distribution—an unrealistic assumption to make in many
situations.</p>

<p>24 The demonstration itself provides ample practice for
most students.  I do assign a few exercises to be done by
hand; I supply sorted data and call for exactly attainable
levels.  I touch on the idea of being conservative when
levels are not so convenient and briefly mention
interpolation. Beyond this it is assumed that software will
do the computing. (A Minitab macro for obtaining one-sample
PIs is provided in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#AppendixB">Appendix B</a>.)</p>

<p>25 The application of this PI as a formal inference
requires randomly sampled data.  In any case, however, this
PI does form an (approximate) statement about the observed
data, as approximately <nobr>(1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">) × 100%</nobr>

of the sample data lies within the bounds.  I believe that
this is a relevant observation to make with students—a
procedure that aims to predict a future value at  level 
<nobr>(1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">) × 100%</nobr>

ought to predict the observed data at at least
approximately this level.  (The prediction perspective
clarifies the slight disparity—we predict the gaps.
However, there is one gap per observation, with but a
single gap left over.  As a result, including gaps is
roughly equivalent to including observations.) Ignoring the
finer points then, we have connections between the PI, the
related descriptive measure of percentile (discussed in the
next section), the graphical device(s) used to display the
data, and, in some cases, the empirical rule.</p>


<h2><a name="Section32">3.2 An Associated Topic: Percentiles </a></h2>

<p>26 The treatment of PIs presented in this article results
in no small part from my desire to create more uses for
percentiles. Percentiles are fundamental descriptive
measures that I expect students to master.</p>

<p>27 Informally, prediction bounds may be viewed as
estimates of the population percentiles that form PPI
bounds (<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Vardeman94">Vardeman 1994</a>, p. 339,  and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#McClave">McClave, Dietrich, and Sincich 1997</a>, p. 516,
implicitly suggest this
relationship). Defining percentiles appropriately results
in a unity across statistical intervals (PPIs, PIs, and
many CIs): use—at least in part—percentiles to
obtain interval bounds.</p>

<p>28 Return to the example cited above, where the sample
size is 19. Again use the 20 gaps to divide the whole into
parts.  In case of a random sample, since on average 5% of
the unsampled observations lie below the minimum sample
value, it is reasonable to take the minimum to be the
(sample) 5<sup>th</sup> percentile. To generalize this
result, formally define sample percentiles as follows  
(<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Gumbel">Gumbel 1939</a>, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Weibull">Weibull 1939</a>). </p>


<!-- MATH: \begin{equation}
Y_{\left( j \right)}, {\mbox{ the $j^{\mbox{{\scriptsize {th}}}}$\space order
statistic, is the $\frac{j}{ n + 1} \times
100^{\mbox{{\scriptsize {th}}}}$\space percentile}}.
\end{equation} -->

<a name="Equation4"></a><div align="CENTER"><a name="Equation4"></a>

<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="463" height="36" src="./preston_8_3_files/preston.img30.gif" alt="\begin{displaymath}
Y_{\left( j \right)}, {\mbox{ the $j^{\mbox{{\scriptsize {th...
...1} \times
100^{\mbox{{\scriptsize {th}}}}$\space percentile}}.
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(4)</td></tr>
</tbody></table>

</div>

<p>(A Minitab macro for
obtaining these percentiles is provided in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#AppendixB">Appendix B</a>.)</p>

<p>29 Appealing to the visual devices shown in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure1">Figures 1</a> and
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure2">2</a>, students readily associate the bounds of PIs with
appropriate percentiles. For example, to obtain a 50%
(two-sided) PI, use as bounds the 25<sup>th</sup> and
75<sup>th</sup> percentiles.  Having discovered this, we
quickly reverse direction: from this point forward
appropriate percentiles are used to form both one- and
two-sided interval bounds.</p>

<p>30 Other definitions exist for sample
percentiles—
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Hyndman">Hyndman and Fan (1996)</a> discuss a number
used in various statistical software packages. Using 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation4">(4)</a> in
particular allows for a rigorous association between
percentiles and prediction bounds (the rigor pleases me;
I'm not certain it matters much to my students). When
relying on software that does not make use of 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation4">(4)</a>, PIs
constructed from percentiles do not exactly satisfy
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation2">(2)</a>.</p>


<h2><a name="Section33">3.3 Applying Prediction Intervals
in Simple Linear Regression</a></h2>

<p>31 It is now common for a descriptive treatment of simple
linear regression to appear early in an introductory
statistics course. In treating the situation, I find it
useful to apply the nonparametric PI described above. 
(This is not the familiar version of the PI. Further, the
procedure I describe here satisfies 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation2">(2)</a> only
approximately.)  I developed the exercise demonstrated
below in order to make constructive use of residuals in
exposing the fundamental utility of regression: account for
variables that affect the response, thereby reducing error
and increasing precision in estimating the response.</p>

<p>32 I illustrate with an example.  Consider predicting
average daily electrical consumption <i>Y</i>, in kilowatt
hours per day, given size of residence <i>x</i>, in square
feet (data from <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Graybill">Graybill, Iyer, and Burdick 1998, pp.
256-257</a>).  Appropriate plots are shown in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure3">Figure 3</a>. I ask:
Given a residence of 1920 square feet, how do we go about
forming a 50% PI for the average daily electrical
consumption at that residence?  Again students take part in
developing a method of solution. To begin, I suggest they
obtain a 50% PI for electrical consumption—ignoring
for the time being residence size. We agree that adjusting
our prediction for residence size will improve predictive
accuracy. The discussion centers how to achieve this;
students have fairly good ideas—typically identifying
ends rather than means. It becomes clear that some new
tactics are required.  We proceed in a fashion that is
central to statistical practice: isolate residual from fit.
(Statistical software handles the computationally intensive
tasks described below.)</p>


<ol>

<p></p><li>Produce a scatterplot. Identify the association as a
linear one; check for outliers and leverage values. <p></p>

<p></p></li><li>Obtain and plot a fitted line (a resistant fit
suffices).  Here I've used the least squares line.  At this
point, hard copies are distributed; students augment the
plot by hand.<p></p>

<p></p></li><li>Make certain that the equation and plotted line
agree by computing the fit at 1920,<p></p>

<center>
-13.577 + .033085 × 1920 = 49.943,
</center>

<p>marking it on the plot. Agree that this value makes a
good anchor for our interval.</p>

<p></p></li><li>Obtain the residuals.  Make certain we understand
what the residuals represent graphically— validating
for a few cases that DATA = FIT + RESIDUAL. Produce
residual plots.<p></p>

<p></p></li><li>Obtain appropriate percentiles of the residual
distribution. For a 50% PI we need the first and third
quartiles; they are -3.066 and 3.755, respectively.<p></p>

<p></p></li><li>Apply percentiles of the residuals to the fit to
form the PI: the lower bound is 49.943 + (-3.066) = 46.877;
the upper bound is  49.943 + 3.755 = 53.698.  The PI for
energy consumption at a residence of 1920 square feet is
(46.9 kwh/d, 53.7 kwh/d).<p></p>

<p></p></li><li>Mark this interval with a vertical bar on the
plot.<p></p>

<p></p></li><li>Repeat steps 3, 6, and 7 for a variety of residence
sizes ranging from 1500 to 2400 square feet (this task is
distributed among students).<p></p>

<p></p></li><li>A number of PIs are now plotted as vertical bars
over a variety of explanatory levels.  Connect the tops
(bottoms) of these vertical bars to obtain an upper (lower)
prediction limit line.  The resulting lines form an
(approximate) <i>prediction band</i>.<p></p>

<p></p></li><li>A confirmatory count reveals that, as it should be,
approximately 50% of the observations lie within this
band.<p></p>

</li></ol>

<p>In this example the resulting prediction band is nearly
coincident with that obtained by the traditional method
given by <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation5">(5)</a> in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section4">Section 4</a>.</p>



<p><a name="Figure3"></a></p><hr><p></p>
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston03.gif">
<img src="./preston_8_3_files/preston03.icon.jpg">Figure 3 (10.0K gif)</a>

<p> Figure 3. Applying the Nonparametric PI in Simple
Linear Regression. A scatterplot of energy consumption
versus size of residence for 40 residences is displayed.
The least squares fit and boxplots for both energy
consumption (univariate) and the residuals are shown. A
one-sample nonparametric 50% PI for the residuals is
bounded by the first and third quartiles.  This interval is
then applied to the fitted line to obtain a prediction
band. 50% of the observations lie within this band.</p>

<p></p><hr><p></p>


<p>33 Regression is motivated as a reduction of error
technique. To illustrate this, I make use of a summary
statistic analogous to the coefficient of determination
<i>r</i><sup>2</sup> and derived entirely from prediction
statements.  Continuing with the example, the 50% PI for
consumption unadjusted for residence size is  (41.00,
56.25), with width  56.25 - 41.00 = 15.25. Next obtain the
width of the 50% PI for the residuals:  3.755 - (-3.066) =
6.821. The ratio of these widths,  6.821/15.25 = 0.447, is
the fraction of original PI width for estimating energy
usage that remains after fitting the line—after
adjusting for size of residence.  Over half of the
prediction error in  electricity consumption is accounted
for by residence size. (Denote by <i>w</i> this ratio of PI
widths.  For bivariate normal data and a least squares fit,
<i>w</i> and <i>r</i><sup>2</sup> are related: 

<img width="142" height="45" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img37.gif" alt="$\left(
1 - w^2 \right) / r^2 \stackrel{p}{\rightarrow} 1$">.

In the example,  <nobr>1 - <i>w</i><sup>2</sup> =
0.800</nobr> and  <nobr><i>r</i><sup>2</sup> =
0.764.)</nobr></p>

<p>34 This exercise forces students to make use of a number
of important definitions and techniques: fitting a line,
obtaining and examining residuals, and finding appropriate
percentiles. Both question and solution are addressed by
the graphical device.</p>

<dl>
<p></p><dt><strong>Question:</strong>

</dt><dd>[The scatterplot is produced …] A 50% PI should
predict approximately 50% of the observed data.  What must
be done to accomplish this?<p></p>

<p></p></dd><dt><strong>Solution:</strong>

</dt><dd>[The bounds are plotted and …] approximately 50% of
the observations fall within the band. <p></p>

</dd></dl>

(This sort of motivation/validation is not possible with
confidence bounds.)  My students find this exercise
satisfactory; it builds on previously defined concepts in
support of new ideas—in particular residuals—and
does so in a straightforward fashion.  The results please
them and agree with their a priori conjectures.<p></p>


<h2><a name="Section34">3.4 Robustness</a></h2>

<p>35 The one-sample nonparametric PI requires only a random
sample of observations from a continuous-valued
population.  As the demonstration of <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section31">Section 3.1</a>
illustrates, the procedure may be used when randomly
sampling without replacement from a finite
population—provided there are no ties. In practice
there may be ties; generalizations to such situations are
possible, but in my view are not worth detailing to an
introductory audience.  As with other inferential
procedures, this PI is not robust against a violation of
the assumption of randomly sampled data.</p>

<p>36 The procedure I describe in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section33">Section 3.3</a> does not
produce an exact PI.  In the general linear model, suppose
residual distributions are identical across treatment
levels.  Forming a nonparametric PI with the pooled
residuals, then applying the resulting interval to
estimated treatment means (fits) to form PIs at specific
treatment levels, does not exactly satisfy the definition
of a PI given by <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation2">(2)</a>
—it does so only approximately for
large sample sizes, because sampling variability in the
estimated means is not accounted for.  The normal theory PI
is an alternative—provided residual structure is
normal. More generally, "…the theory for statistical
prediction intervals is complicated" (<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Hahn91">Hahn and Meeker 1991,
p. 242</a>). The definition is exactly satisfied in
multi-sample situations provided one does <i>not</i> pool,
and forms instead a PI at a given treatment level using the
one-sample method with only the data at that level. Such an
attack is robust against any violation of the identical
distributions across treatment levels assumption. (I find
the regression application illustrated above far more
useful and do not treat analysis of variance in a similar
fashion.)</p>

<p>37 In regression applications, it is instructive to
demonstrate how the procedure fails in cases of nonconstant
variance.  To illustrate, consider the data displayed in
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure4">Figure 4</a> 
(data from <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Neter">Neter et al. 1996, p. 407</a>). Systolic
blood pressure of healthy women is regressed on age. The
50% prediction band constructed using the method of the
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section33">Section 3.3</a> contains—by
definition—(approximately) 50% of the data. Yet
students who have seen the band plotted on a scatterplot
will argue against using the intervals— particularly if
it is emphasized that the objective is a procedure that
achieves 50% confidence at each age. In short: Don't use
the method to predict unless appropriate assumptions are
satisfied.  A good lesson!</p>



<p><a name="Figure4"></a></p><hr><p></p>
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston04.gif">
<img src="./preston_8_3_files/preston04.icon.jpg">Figure 4 (5.2K gif)</a>

<p> Figure 4. The PI Fails when Variance is Not Constant. A
scatterplot of systolic blood pressure versus age, for
healthy women, is displayed.  The nonparametric PI is
applied with the residuals to obtain a 50% prediction
band.  Because of nonconstant error variance, the band
fails to predict at the 50% level for all ages.</p>

<p></p><hr><p></p>


<h1><a name="Section4">4. Normal Theory Prediction Intervals</a></h1>


<p>38 In the classical model for simple linear regression,
the appropriate PI for a future value <i>Y</i>, given
explanatory level 

<img width="23" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img39.gif" alt="$x^{\star}$">,
is</p>

<a name="Equation5"></a><div align="CENTER"><a name="Equation5"></a>

<!-- MATH: \begin{equation}
\hat{y} \pm t_{\alpha/2} s \sqrt{ 1 + \frac{1}{n} + \frac{
\left(x^{\star} - \overline{x} \right)^2 } {\sum_{i=1}^{n} \left(x_i -
\overline{x} \right)^2 } },
\end{equation} -->

<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="282" height="65" src="./preston_8_3_files/preston.img40.gif" alt="\begin{displaymath}
\hat{y} \pm t_{\alpha/2} s \sqrt{ 1 + \frac{1}{n} + \frac{
\...
...t)^2 } {\sum_{i=1}^{n} \left(x_i -
\overline{x} \right)^2 } },
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(5)</td></tr>
</tbody></table>

</div>

<p> where <i>s</i><sup>2</sup> is the estimated residual
variance. This PI should be computed by software. If a
formula is presented, it ought to be one that resorts to
minimal notation while still capturing the essence of the
method and providing insight into the result.  It is also
desirable that the procedure be easily generalized to a
variety of models, in much the same way as are confidence
interval and significance testing procedures.</p>

<p>39 In this section, I suggest a simple, interpretable,
alternative formulation for normal theory PIs such as 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation5">(5)</a>. 
This alternative may be applied in each of the standard
models taught in an introductory course— provided the
proper assumptions are met. Because the procedure does not
share the well-known robustness properties of corresponding
confidence interval and significance testing procedures,
the section closes with a brief discussion of its
robustness.</p>



<h2>4.1 A General Expression for Normal Theory Prediction
Intervals </h2>

<p>40 The standard error of <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation5">(5)</a> may be recast in friendly
and generalizable terms.  Assume the classical model where
the observed random variables are independent normal with
common variance.  The expected value of each observation is
perhaps a function of explanatory variables.  Take <img width="15" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img41.gif" alt="$\hat{y}$">

and <i>s</i><sup>2</sup> to be the usual estimates of the
mean (expected value) and variance of a subsequent variable
<i>Y</i>. Denote by s.e.(<img width="15" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img41.gif" alt="$\hat{y}$">)

the estimated standard deviation (standard error) of the
estimate <img width="15" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img41.gif" alt="$\hat{y}$">.

Define the <i>standard error of prediction</i> for <i>Y</i>
by </p>

<a name="Equation6"></a><div align="CENTER"><a name="Equation6"></a>

<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="220" height="36" src="./preston_8_3_files/preston.img43.gif" alt="\begin{displaymath}
\mbox{s.e.p.} \left(Y \right) = \sqrt{ s^2 + \mbox{s.e.}^2 \left(
\hat{y} \right) }.
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(6)</td></tr>
</tbody></table>

</div>

<p> A PI for <i>Y</i> is then formed by </p>

<a name="Equation7"></a><div align="CENTER"><a name="Equation7"></a>

<table width="100%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="144" height="35" src="./preston_8_3_files/preston.img44.gif" alt="\begin{displaymath}
\hat{y} \pm t_{\alpha/2} \mbox{s.e.p.} \left( Y \right),
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(7)</td></tr>
</tbody></table>

</div>

<p>using the appropriate error degrees of freedom. (This
formulation is merely a re-expression of the normal theory
PI that appears in texts. It is well known.)</p>

<p>41  Such a PI is formed out of fundamental quantities:
<img width="15" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img41.gif" alt="$\hat{y}$">,

s.e.(<img width="15" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img41.gif" alt="$\hat{y}$">),

and <i>s</i> are readily obtained using statistical
software.  When applied in a specific model, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> yields to
algebraic simplification (because  s.e.(<img width="15" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img41.gif" alt="$\hat{y}$">) = <i>cs</i>

for some positive constant <i>c</i>), but the
simplification really misses the point: This scheme
<i>always works the same way</i>.</p>

<ul>
<p></p><li>
<b>One-sample model.</b> Here 

<img width="50" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img46.gif" alt="$\hat{y} = \overline{y}$">,
so 

<img width="207" height="38" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img47.gif" alt="$\mbox{s.e.}
\left(\hat{y} \right) = \mbox{s.e.}
\left(\overline{y} \right) = s / \sqrt{n}$">.

Then <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> <i>may</i> be expressed as<p></p>

<a name="Equation8"></a><div align="CENTER"><a name="Equation8"></a>

<table width="93%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="276" height="56" src="./preston_8_3_files/preston.img48.gif" alt="\begin{displaymath}
\mbox{s.e.p.} \left(Y \right) = \sqrt{ s^2 + \frac{s^2}{n}} = s
\sqrt{1 + \frac{1}{n}} .
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(8)</td></tr>
</tbody></table><p></p>

</div>

<p></p></li><li>

<b>Multi-sample models, assumed equal variance.</b> Samples
of sizes <i>n<sub>j</sub></i>&nbsp;, <i>j</i> = 1, 2,…, <i>k</i>, are obtained for each of <i>k</i> groups.
Assume a prediction is desired for an observation in group
<i>j</i>. The estimated mean is 

<img width="57" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img50.gif" alt="$\hat{y} = \overline{y}_j$">,
so 

<img width="142" height="37" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img51.gif" alt="$\mbox{s.e.}
\left(\hat{y}_j \right) = s / \sqrt{n_j}$">,

where <i>s</i> is the pooled standard deviation. <p></p>

<center>
<img width="282" height="65" src="./preston_8_3_files/preston.img52.gif" alt="\begin{displaymath}\mbox{s.e.p.} \left(Y \right) = \sqrt{ s^2 + \frac{s^2}{n_j}} = s
\sqrt{1 + \frac{1}{n_j}}
\end{displaymath}">
</center>

<p></p></li><li> <b>Simple linear regression.</b>  Since <p></p>


<!-- MATH: \begin{equation}
{\mbox{s.e.}} \left( \hat{y} \right)
=s \sqrt{ \frac{1}{n} +
\frac{\left( x^{\star} - \overline{x} \right)^2}{\sum_{i=1}^{n}
\left(x_i - \overline{x} \right)^2}},

\end{equation} -->

<a name="Equation9"></a><div align="CENTER"><a name="Equation9"></a>

<table width="93%" align="CENTER">
<tbody><tr valign="MIDDLE"><td align="CENTER" nowrap=""><img width="265" height="65" src="./preston_8_3_files/preston.img53.gif" alt="\begin{displaymath}
{\mbox{s.e.}} \left( \hat{y} \right)
=s \sqrt{ \frac{1}{n} +...
...right)^2}{\sum_{i=1}^{n}
\left(x_i - \overline{x} \right)^2}},
\end{displaymath}"></td>
<td width="10" align="RIGHT">
(9)</td></tr>
</tbody></table>
<p></p>

</div>

<p>s.e.p.(<i>Y</i>) is as given by <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation5">(5)</a>.</p>
</li></ul>

<p>Similar expressions are readily developed for other
models.</p>

<p>42 Take, for example, the one-sample normal model: The
goal is to use the observed data to make a statement
analogous to the PPI given by <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation1">(1)</a>. Adopting the view of PI
bounds as estimates of PPI bounds, heuristic justification
for the interval formed by applying 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a>, and then 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation7">(7)</a>,
might run as follows.</p>


<ol>

<p></p><li>
<img width="16" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img9.gif" alt="$\sigma$">
quantifies unit-to-unit variability.  Because it is
unknown, replace it with the estimate <i>s</i>. <p></p>

<p></p></li><li>Estimating <img width="16" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img9.gif" alt="$\sigma$">
with <i>s</i> entails the use of the <i>t</i>-distribution.
<p></p>

<p></p></li><li>Because the mean <img width="16" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img8.gif" alt="$\mu$">
is unknown, estimate it with 

<img width="15" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img55.gif" alt="$\overline{y}$">.
<p></p>

<p></p></li><li>
<img width="15" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img55.gif" alt="$\overline{y}$">
is a variable; the sampling variability in

<img width="15" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img55.gif" alt="$\overline{y}$">
is quantified by s.e.(<img width="15" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img55.gif" alt="$\overline{y}$">).
<p></p>

<p></p></li><li>The formula <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> 
for s.e.p.(<i>Y</i>) reflects the
statistician's way of combining standard deviations from
independent sources (that is, the variances are summed).
<p></p> 
</li></ol>

<p>43 As with the nonparametric method, this PI is validated
by plotting prediction limits along with the data. 
Implemented when appropriate assumptions are met,
approximately (1 - <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$">)

of the data should fall within the bounds.  Comparing
prediction bounds to similarly plotted confidence bounds
effectively contrasts the two procedures.</p>

<p>44 Few textbooks make use of this approach (I have yet to
see it appear in an introductory text of the sort I would
consider using).  A pleasantly surprising number of my
colleagues (all mathematicians) are aware of 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> within the
scope of regression.</p>

<p>45 An acceptable approach to teaching normal theory PIs
is to ignore formulas altogether and let software do the
computing (<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Graybill">Graybill et al. 1998, pp. 232-234</a>, take this
route).  The essential properties of PIs are readily
demonstrated by covering the nonparametric alternative of
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section31">Section 3.1</a>.  Recognize, however, that use of 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> with 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation7">(7)</a>
is restricted to situations in which the assumptions of the
classical normal theory models are strictly met.</p>



<h2><a name="Section42">4.2 Robustness</a></h2>

<p>46 The normal theory PI is robust against violations of
<i>none</i> of the assumptions of the classical normal
theory models.</p>

<p>47 The procedure fails to achieve nominal levels when
observations are not normally distributed.  Central limit
theory does not apply here as it would for, say, a
confidence interval.  The informal argument presented above
cannot even begin: There is no 

<img width="82" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img57.gif" alt="$\mu \pm z_{\alpha/2}
\sigma$">

to estimate. Textbooks typically handle this issue in
regression by stating an assumption that residual structure
is normal. I would caution that simply stating the
assumption does not imply its truth in application. 
Deviations from normality often occur in the tails of a
distribution.  If this is the case, the normal theory PI
will be in error when constructed at high levels 
(<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Hahn70">Hahn 1970, p. 201</a>). If a transformation is unnecessary, or fails
to produce normal error structure, the nonparametric PI of
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section3">Section 3</a> remains as an (at least approximate)
alternative.</p>

<p>48 Nonconstant variance is common in comparative models. 
Transforming the response often results in (essentially)
normal, equal-variance residual structure.  A PI obtained
for transformed data may be inverse transformed into
original units with no compromise of level (this is not the
case for a confidence interval).</p>

<p>49 In multi-sample situations calling for mean inference,
pooling is acceptable—although perhaps not
desirable—even in the presence of unequal variance,
provided sample sizes within groups are approximately equal
(<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Moore">Moore and McCabe 1999, p. 554</a>). Such a result in no way
extends to the PI. Given normal data, but unequal treatment
group variances, the appropriate attack is to use a
within-group one-sample PI.</p>



<h1><a name="Section5">5. Connections</a></h1>

<p>50 Two methods for obtaining PIs have been presented in
this article; comparing them explains the workings of the
normal probability plot.</p>

<p>51 Consider the one-sample model.  For a variety of <img width="17" height="17" align="BOTTOM" border="0" src="./preston_8_3_files/preston.img3.gif" alt="$\alpha$"> values,

plot the lower and upper endpoints of the nonparametric PI
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation3">(3)</a> against the corresponding endpoints of the normal
theory PI given by <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation7">(7)</a> and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a>.  When observations are
randomly sampled from a normal population, the two
procedures should produce similar results uniformly over
all levels. Consequently, a plot of nonparametric PI bound
(data) versus  normal theory PI bound will be generally
linear. (This is not precisely the normal probability plot.
Percentiles of the <i>t</i> distribution are employed,
rather than those of the standard normal.)</p>

<p>52 To illustrate, consider the component failure time
data in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Table1">Table 1</a>. 
The respective intervals are presented in
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Table2">Table 2</a>.  The suggested plot is shown in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure5">Figure 5</a>.</p>


<a name="Table2"><p></p><hr><p></p></a>


<p><b>Table 2. </b>Nonparametric and Normal Theory
Prediction Bounds for Component Failure Times</p>


<table cellspacing="1" cellpadding="6" border="4">
<tbody><tr><td align="RIGHT">&nbsp;</td>
<td align="CENTER" colspan="4">Prediction Bounds</td>
</tr>
<tr><td align="RIGHT">&nbsp;</td>
<td align="CENTER" colspan="2">Lower</td>
<td align="CENTER" colspan="2">Upper</td>
</tr>
<tr><td align="RIGHT">Level</td>
<td align="CENTER">Nonparametric</td>
<td align="CENTER">Normal</td>
<td align="CENTER">Nonparametric</td>
<td align="CENTER">Normal</td>
</tr>
<tr><td align="RIGHT">90%</td>
<td align="CENTER">32.56</td>
<td align="CENTER">39.46</td>
<td align="CENTER">87.87</td>
<td align="CENTER">86.00</td>
</tr>
<tr><td align="RIGHT">80%</td>
<td align="CENTER">42.02</td>
<td align="CENTER">44.88</td>
<td align="CENTER">80.37</td>
<td align="CENTER">80.58</td>
</tr>
<tr><td align="RIGHT">70%</td>
<td align="CENTER">47.26</td>
<td align="CENTER">48.41</td>
<td align="CENTER">76.33</td>
<td align="CENTER">77.05</td>
</tr>
<tr><td align="RIGHT">60%</td>
<td align="CENTER">50.25</td>
<td align="CENTER">51.16</td>
<td align="CENTER">71.27</td>
<td align="CENTER">74.30</td>
</tr>
<tr><td align="RIGHT">50%</td>
<td align="CENTER">59.03</td>
<td align="CENTER">53.49</td>
<td align="CENTER">70.60</td>
<td align="CENTER">71.97</td>
</tr>
<tr><td align="RIGHT">40%</td>
<td align="CENTER">60.17</td>
<td align="CENTER">55.57</td>
<td align="CENTER">68.71</td>
<td align="CENTER">69.89</td>
</tr>
<tr><td align="RIGHT">30%</td>
<td align="CENTER">61.56</td>
<td align="CENTER">57.48</td>
<td align="CENTER">66.54</td>
<td align="CENTER">67.98</td>
</tr>
<tr><td align="RIGHT">20%</td>
<td align="CENTER">62.16</td>
<td align="CENTER">59.28</td>
<td align="CENTER">65.52</td>
<td align="CENTER">66.18</td>
</tr>
<tr><td align="RIGHT">10%</td>
<td align="CENTER">62.84</td>
<td align="CENTER">61.02</td>
<td align="CENTER">63.52</td>
<td align="CENTER">64.44</td>
</tr>
<tr><td align="RIGHT">0%</td>
<td align="CENTER">63.29</td>
<td align="CENTER">62.73</td>
<td align="CENTER">63.29</td>
<td align="CENTER">62.73</td>
</tr>
</tbody></table>


<p>NOTE: The nonparametric bounds are simply the 
data presented in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Table1">Table 1</a>. The normal theory bounds are
obtained using 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation7">(7)</a> with  <img width="15" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img55.gif" alt="$\overline{y}$">  = 62.730 and <i>s</i> = 13.077.</p>


<p><a name="Figure5"></a></p><hr><p></p>
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston05.gif">
<img src="./preston_8_3_files/preston05.icon.jpg">Figure 5 (5.8K gif)</a>

<p> Figure 5. Plotting Corresponding Prediction Interval
Bounds. A plot of nonparametric prediction bounds versus
normal theory counterparts is shown for the component
failure time data. The bounds are listed in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Table2">Table 2</a>. A
normal probability plot is superimposed:
<img width="157" height="46" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img59.gif" alt="$
\left( \Phi^{-1} \left( \frac{i + 3/8 }{ n + 1/4 } \right), 
y_{\left(i \right)} \right) $">.</p>

<p></p><hr><p></p>




<p>53 Comparing the two PIs leads to a natural question:
When treating normally distributed data, how do these two
prediction procedures compare?  Given normal residual
structure, the normal theory PI is preferred over the
nonparametric PI (<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Hahn91">Hahn and Meeker 1991, pp. 75-76</a>). 
However, <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Patel">Patel (1989, p. 2401)</a> points out that there is
little available information on this issue. Comparing the
two methods (perhaps via simulation) seems a fruitful area
of investigation for students in, say, mathematical
statistics courses. Indeed, even the question "How should
they be compared?" is worth taking up with an audience
predisposed towards theory.</p>



<h1><a name="Section6">6. Summary</a></h1>

<p>54 Prediction intervals are a success in my classroom. 
Students like this relatively concrete application; working
with PIs builds confidence and provides learners with a
measure of insight into a number of statistical
procedures.  With a PI, the data are used to make a formal
inference in a way that is non-trivial (yet often
computationally simple), is directly referenced by
graphically displayed data, and makes use of fundamental
statistical techniques. Finally, PIs form somewhat of a
rallying point for my instruction.  I find myself
constantly referring to them in order to illustrate a
variety of issues.  Try teaching prediction intervals!</p>

<p></p><hr><p></p>


<h1><a name="AppendixA">Appendix A:  Burn Times of Matches</a></h1>


<p> The activity referred to in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section31">Section 3.1</a> requires of
students a little time (say two minutes each), and of the
instructor an extremely small investment in supplies.  I
use Ohio Blue Tip Kitchen Matches. The matches are notched
about 5 mm from the bottom.  Using either pliers or a vice,
grip the match at the notch (assuring uniformity in the way
the matches are positioned), orienting the match upright. 
A lighter ignites the match—ignition is marked by a
very distinctive sound that triggers the start of a
stopwatch. When the flame dies—again quite
identifiable and accompanied by a distinctive wisp of
smoke—the watch is stopped and the elapsed time
recorded.</p>

<p> I prefer to have all students perform this experiment
in the same conditions.  My office is equipped with a
makeshift vice, lighter, stopwatch, and a box of matches.
Students stop by, I supply a few directions, a match is
selected and set upright in the vice, I apply the lighter
to the match head, and, upon ignition, the student begins
timing.  When the flame dies (the current low/high records
are about 15/90 seconds), the watch is stopped, and the
student records the burn time of the match to the nearest
0.01 second (overkill, but it prevents ties).</p>

<p> While the particular collection of 19 observations
given in <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Table1">Table 1</a> show little evidence of nonnormality (see
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure5">Figure 5</a>), the larger collection I have accumulated
markedly departs from normality in the form of substantial
left skew.</p>

<p>

</p><h1><a name="AppendixB">Appendix B: Minitab Macros</a></h1>

<h2>B.1 Prediction Intervals </h2>

<p>The macro described below obtains two-sided PIs for
univariate data in Minitab column C.  I assume the macro is
saved in the file <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/predict.mac">predict.mac</a>.
Invoke it with

</p><pre>        MTB &gt; %predict C P
</pre>

where P% is the confidence level. Six PIs are reported:<p></p>

<ol>

<p></p><li> A conservative exact two-sided nonparametric PI, at
the smallest attainable level no less than P%.<p></p>

<p></p></li><li>A liberal exact two-sided nonparametric PI, at the
largest attainable level no greater than P%.<p></p>

<p></p></li><li>An exact PI at level between those of the
conservative and liberal PIs, formed with the lower
endpoint of the conservative PI and upper endpoint of the
liberal PI.  This interval is two-sided but unbalanced by
one position.<p></p>

<p></p></li><li>An exact PI at level between those of the
conservative and liberal PIs, formed with the lower
endpoint of the liberal PI and upper endpoint of the
conservative PI.  This interval is two-sided but unbalanced
by one position.<p></p>

<p></p></li><li>An (approximate) P% two-sided PI, obtained by linear
interpolation of the bounds given by the PIs of 1 and 2
above.<p></p>

<p></p></li><li>The P% normal theory two-sided PI.<p></p>
</li></ol>

<p> If an exact, balanced, two-sided nonparametric PI is
attainable at the given level, it alone is produced in lieu
of 1 through 5.



</p><h2>B.2 Percentiles</h2>

<p> A second macro obtains a P<sup>th</sup> percentile for
univariate data in Minitab column C.  I assume the macro is
saved in the file <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/pctile.mac">pctile.mac</a>.
Invoke it with

</p><pre>        MTB &gt; %pctile C P
</pre>

<p>The definition of <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section32">Section 3.2</a> is used to obtain the two
nearest exact percentiles, as well as a linearly
interpolated value for the P<sup>th</sup> percentile. If
the percentile is exactly attainable, it alone is produced.
The macro is easily adapted to obtain one-sided prediction
intervals.</p>


<h1><a name="AppendixC">Appendix C: Textbook Coverage of
Prediction</a></h1>

<p> The textbooks I examined while preparing this article
try very hard to get prediction right.  My argument is
rarely with what they do say, instead it is with how they
say it and what they fail to mention. Often, considering a
clientele of undergraduates with little statistical
experience, the prose is quite difficult.  A brief report
of my observations follows.</p>

<p> The one-sample normal theory PI is formulated in
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Scheaffer">Scheaffer and McClave (1990, p. 291)</a>, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Devore">Devore (1995, p. 296)</a>, and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Vining">Vining (1998, p. 176)</a>, using the right-most expression
of <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation8">(8)</a>.  These texts, as well as 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Vardeman94">Vardeman (1994)</a> (discussed
below), are calculus-based texts that target engineers.</p>

<p> Extensive coverage of prediction intervals, as well as
tolerance intervals, is found in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Vardeman94">Vardeman (1994)</a>.  The
nonparametric PI of <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Section31">Section 3.1</a> is covered on pages
344-347.  Treatment is confined to the minimum and maximum
as bounds.  This is reasonable when data are collected on a
system operating in control; the extremes form natural
control limits, and determining the confidence level 
achieved when using these limits is then important.  The
text also treats normal theory PIs for a number of models. 
While <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> 
is not implemented throughout, it does appear on
page 553, in a discussion of how to construct a PI from
software regression results. The text includes a wealth of
PI exercises.</p>

<p> The expression for standard error of prediction given
by <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> appears in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Dielman">Dielman (1991, p. 109)</a>—not an
introductory text.</p>

<p> Either <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation5">(5)</a>, or something very similar, is found almost
without exception in introductory statistics texts written
with a broad, nonmathematically-oriented audience in mind.
To list but a few: 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#McClave">McClave et al. (1997, p. 511)</a>, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Triola">Triola (1997, p. 513)</a>, 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Blaisdell">Blaisdell (1998, p. 664)</a>, and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Weiss">Weiss (1999, p. 905)</a>; 
also—in an optional section on regression computations—
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Graybill">Graybill et al. (1998, p. 267)</a> and 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Moore">Moore and McCabe (1999, p. 690)</a>.  All of  these cited
texts—and many others—limit the treatment of
prediction to linear regression.</p>

<p> I found one example of inappropriate statistical
practice.  An example in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Weiss">Weiss (1999, pp. 872-908)</a>
regresses the price of the <i>Orion</i> model of automobile
on age.  A normal probability plot of the residuals is
shown in 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Figure6">Figure 6</a>; this plot is duplicated on page 881 of
the text.  The normal theory PI 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation5">(5)</a> is used to predict the
price of a three-year-old Orion.  The discussion that
accompanies the probability plot concludes "There are no
obvious violations of the assumptions for regression
inferences" (p. 881).  I disagree: there <i>is</i> evidence
that the residual structure is nonnormal.  I am not at all
convinced that regression inferences are
appropriate—particularly predictive inferences. (It is
questionable whether mean inference—using the usual
methods—ought to be undertaken here. Certainly the
nominal procedural levels must be taken with a grain of
salt.)</p>



<p><a name="Figure6"></a></p><hr><p></p>
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston06.gif">
<img src="./preston_8_3_files/preston06.icon.jpg">Figure 6 (6.2K gif)</a>

<p> Figure 6. Normal Probability Plot of Residuals. Price
of the  Orion model automobile is regressed on age; a
normal probability plot of the residuals is displayed.</p>

<p></p><hr><p></p>



<p> Textbooks take pains to distinguish the prediction
interval from the confidence interval. The following is
exemplary, and would work to even greater effect had 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a>
appeared in the accompanying section on regression
calculations.</p>

<blockquote>

…the standard error…used in the prediction
interval includes both the variability due to the fact that
the least-squares line is not exactly equal to the true
regression line <i>and</i> the variability of the future
response variable…around the subpopulation mean.
(<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Moore">Moore and McCabe 1999, p. 676</a>)

</blockquote>

<p>Isn't this exactly what <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation6">(6)</a> states?</p>

<p> <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Graybill">Graybill et al. (1998)</a> omit the traditional PI formula
from the text proper, including it only in an appendix. 
This text's notation distinguishes between the prediction
function, denoted <i>Y(x)</i>, and the mean function 

<img width="56" height="37" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img64.gif" alt="$\mu_Y \left(x \right)$">.

It is assumed that students will use statistical software
to do the computations.</p>

<p> <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Triola">Triola (1997, pp. 513 and 517)</a> produces the standard
error of prediction in the text proper, relegating the
standard error of an estimated mean to an exercise.</p>

<p> Explaining the traditional formula 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation5">(5)</a> results in some
interesting reading, particularly when it is contrasted
with the corresponding formula <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Equation9">(9)</a> for the standard error
of an estimated mean.</p>

<blockquote>

Notice that the only difference in the formulas is the
inclusion of a 1 under the radical in the error bound for
the prediction interval. (<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Blaisdell">Blaisdell 1998, p. 664</a>)

</blockquote>

<p>True enough.  Yet the number 1 is hardly
important—it is the application of 1 to the residual
standard deviation that matters. Here's a very similar
statement.</p>

<blockquote>

Note that the only difference between the recipes for these
two standard errors is the extra 1 under the square root
sign in the standard error for prediction.  This standard
error is larger due to the additional variation of
individual responses about the mean response. 
(<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Moore">Moore and McCabe 1999, p. 690</a>)

</blockquote>

<p>The formula for standard error of prediction is then
produced, with a reference directing the reader to the
following note: "This quantity is the estimated standard
deviation of 

<img width="48" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img65.gif" alt="$\hat{y} - y$">,
 
not the estimated standard deviation of <img width="15" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img41.gif" alt="$\hat{y}$">
 
alone" (<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#Moore">Moore and McCabe 1999, p. 709</a>).  While this does
address the quantity (<img width="48" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img65.gif" alt="$\hat{y} - y$">)

a statistician would work with in deriving the PI, I
suspect it in no way guides the intended reader.</p>

<p> The following strikes me as quite difficult reading.

</p><blockquote>

The error in estimating the mean value of <i>y</i>,
<i>E(y)</i>, for a given value of <i>x</i>, say
<i>x</i><sub><i>p</i></sub>, is the distance between the
least square line and the true line of means, 

<img width="142" height="37" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img67.gif" alt="$E \left( y \right) = \beta_0 + \beta_1 x$">.

This error, 

<img width="91" height="37" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img68.gif" alt="$\left[ \hat{y} - E \left( y \right)
\right]$">,

is shown in [a figure not reproduced here].  In contrast,
<i>the error</i>

<img width="69" height="37" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img69.gif" alt="$\left( y_p - \hat{y} \right)$"> 
 
<i>in predicting some future value of y is the sum of two
errors</i>—the error of estimating the mean of
<i>y</i>, <i>E(y)</i> … plus the random error that is
a component of the value of <i>y</i> to be predicted…
(<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#McClave">McClave et al., 1997, pp. 513-514</a>)

</blockquote>

<p> Finally, one incorrect statement.

</p><blockquote>

If you examine the formula for the prediction interval, you
will see that the interval can get no smaller than 

<img width="81" height="34" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img70.gif" alt="$\hat{y} \pm z_{\alpha/2}
\sigma$">.
(<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/secure/v8n3/preston.cfm#McClave">McClave et al. 1997, p. 516</a>)

</blockquote>

<p>While the PI converges in probability to the specified
interval, the observed PI margin of error, 

<img width="115" height="37" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img71.gif" alt="$t_{\alpha/2} {\mbox{ s.e.p.}} \left( Y
\right)$">,

can be less than the population margin of error,

<img width="48" height="33" align="MIDDLE" border="0" src="./preston_8_3_files/preston.img72.gif" alt="$z_{\alpha/2} \sigma$">,

due to sampling variability.</p>


<p></p><hr><p></p>

<h1>References</h1>

<p><a name="Blaisdell"> Blaisdell, E. A. (1998)</a>,
<cite>Statistics in Practice</cite> (2nd ed.), Fort Worth,
TX: Saunders.</p>

<p><a name="Devore"> Devore, J. L. (1995)</a>,
<cite>Probability and Statistics for Engineering and the
Sciences</cite> (4th ed.), Belmont, MA: Duxbury Press.</p>


<p><a name="Dielman">Dielman, T. (1991)</a>, <cite>Applied
Regression Analysis for Business and Economics</cite>,
Boston: Duxbury.</p>


<p><a name="Graybill">Graybill, F. A., Iyer, H. K., and
Burdick, R. K. (1998)</a>, <cite>Applied Statistics: A
First Course In Inference</cite>, Upper Saddle River, NJ:
Prentice Hall.</p>


<p><a name="Gumbel">Gumbel, E. J. (1939)</a>, "La
Probabilité des Hypothèses," <cite>Compes
Rendus de l'Académie des Sciences (Paris)</cite>,
209, 645-647.</p>


<p><a name="Hahn70">Hahn, G. J. (1970)</a>, "Statistical
Intervals for a Normal Population," <cite>Journal of
Quality Technology</cite>, 2, 115-125, 195-206.</p>


<p><a name="Hahn91">Hahn, G. J., and Meeker, W. Q.
(1991)</a>, <cite>Statistical Intervals: A Guide for
Practitioners</cite>, New York, NY: Wiley.</p>


<p><a name="Hyndman">Hyndman, R. J., and Fan, Y.
(1996)</a>, "Sample Quantiles in Statistical Packages,"
<cite>The American Statistician</cite>, 50, 361-364.</p>


<p><a name="McClave">McClave, J. T., Dietrich, F. H., and
Sincich, T. (1997)</a>, <cite>Statistics</cite> (7th ed.),
Upper Saddle River, NJ: Prentice Hall.</p>


<p><a name="Moore">Moore, D. S., and McCabe, G. P.
(1999)</a>, <cite>Introduction to the Practice of
Statistics</cite> (3rd ed.), New York, NY: Freeman.</p>


<p><a name="Neter">Neter, J., Kutner, M. H., Nachtsheim, C.
J., and Wasserman, W. (1996)</a>, <cite>Applied Linear
Statistical Models</cite> (4th ed.), Chicago, IL:
Irwin.</p>


<p><a name="Patel">Patel, J. K. (1989)</a>, "Prediction
Intervals—A Review," <cite>Communications in
Statistics—Theory and Methods</cite>, 18,
2393-2465.</p>


<p><a name="Scheaffer">Scheaffer, R. L., and McClave, J. T.
(1990)</a>, <cite>Probability and Statistics for
Engineers</cite> (3rd ed.), Boston, MA: PWS-Kent.</p>


<p><a name="Scheuer">Scheuer, E. M. (1990)</a>, "Let's
Teach More About Prediction," in <cite>Proceedings of the
Statistical Education Section, American Statistical
Association</cite>, pp. 133-137.</p>


<p><a name="Triola">Triola, M. F. (1997)</a>,
<cite>Elementary Statistics</cite> (7th ed.), Reading, MA:
Addison Wesley.</p>


<p><a name="Vardeman92">Vardeman, S. B. (1992)</a>, "What
About the Other Intervals?," <cite>The American
Statistician</cite>, 46, 193-197.</p>


<p><a name="Vardeman94">----- (1994)</a>, <cite>Statistics
for Engineering Problem Solving</cite>, Boston, MA:
PWS.</p>


<p><a name="Vining">Vining, G. G. (1998)</a>,
<cite>Statistical Methods for Engineers</cite>, Pacific
Grove, CA: Duxbury.</p>


<p><a name="Weibull">Weibull, W. (1939)</a>, "The Phenomena
of Rupture in Solids," <cite>Ingeniors Vetenskaps Akademien
Handlinger</cite>, 153, 17.</p>


<p><a name="Weiss">Weiss, N. A. (1999)</a>,
<cite>Introductory Statistics</cite> (5th ed.), Reading,
MA: Addison Wesley Longman.</p>


<p><a name="Whitmore">Whitmore, G. A. (1986)</a>,
"Prediction Limits for a Univariate Normal Observation,"
<cite>The American Statistician</cite>, 40, 141-143.</p>


<p><a name="Wilks">Wilks, S. S. (1941)</a>, "Determination
of Sample Sizes for Setting Tolerance Limits," <cite>Annals
of Mathematical Statistics</cite>, 12, 91-96.</p>


<p></p><hr><p></p>

<p>Scott Preston<br> 
Department of Mathematics<br>
308 Snygg Hall<br>
SUNY Oswego<br>
Oswego, NY 13126<br>
</p><address>srp@oswego.edu</address><p></p>

<p></p><hr><p></p>

<p align="center"><font face="Arial"><small><a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/index.html">JSE Homepage</a> | <a href="https://web.archive.org/web/20130307063758/https://www.amstat.org/publications/jse/JSEForm.htm">Subscription 
Information</a> | <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/contents.cfm">Current Issue</a> | 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/toc.html">JSE Archive 
(1993-1998)</a> | <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/archive.htm">Data Archive</a> | <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/jindex.html">Index</a> | Search JSE 
| <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/information.html">JSE 
Information Service</a> | <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/board.html">Editorial Board</a> | 
<a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/jse/ifa.html">Information for 
Authors</a> | <a href="https://web.archive.org/web/20130307063758/mailto:journals@amstat.org">Contact JSE</a> | <a href="https://web.archive.org/web/20130307063758/http://www.amstat.org/publications/">ASA 
Publications</a></small></font></p>




<!--
     FILE ARCHIVED ON 06:37:58 Mar 07, 2013 AND RETRIEVED FROM THE
     INTERNET ARCHIVE ON 00:27:18 Nov 03, 2021.
     JAVASCRIPT APPENDED BY WAYBACK MACHINE, COPYRIGHT INTERNET ARCHIVE.

     ALL OTHER CONTENT MAY ALSO BE PROTECTED BY COPYRIGHT (17 U.S.C.
     SECTION 108(a)(3)).
-->
<!--
playback timings (ms):
  captures_list: 61.821
  exclusion.robots: 0.229
  exclusion.robots.policy: 0.221
  RedisCDXSource: 0.989
  esindex: 0.008
  LoadShardBlock: 40.896 (3)
  PetaboxLoader3.datanode: 50.929 (4)
  CDXLines.iter: 16.972 (3)
  load_resource: 61.22
  PetaboxLoader3.resolve: 20.776
--></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>